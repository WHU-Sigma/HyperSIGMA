{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from model import ss_fusion_seg\n",
    "import torch\n",
    "from torch  import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,cohen_kappa_score\n",
    "from model import split_data,utils,create_graph\n",
    "from sklearn import metrics, preprocessing\n",
    "from mmengine.optim import build_optim_wrapper\n",
    "from mmcv_custom import custom_layer_decay_optimizer_constructor,layer_decay_optimizer_constructor_vit\n",
    "import random\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import copy\n",
    "import scipy.io as sio\n",
    "import spectral as spy\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.data_cube = None\n",
    "        self.g_truth = None\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        \"\"\"\n",
    "        origin data\n",
    "        \"\"\"\n",
    "        return self.data_cube\n",
    "\n",
    "    @property\n",
    "    def truth(self):\n",
    "        return self.g_truth\n",
    "\n",
    "    @property\n",
    "    def normal_cube(self):\n",
    "        \"\"\"\n",
    "        normalization data: range(0, 1)\n",
    "        \"\"\"\n",
    "        return (self.data_cube - np.min(self.data_cube)) / (np.max(self.data_cube) - np.min(self.data_cube))\n",
    "class IndianRaw(DataReader):\n",
    "    def __init__(self):\n",
    "        super(IndianRaw, self).__init__()\n",
    "        raw_data_package = sio.loadmat(r\"data/Indian_pines_corrected.mat\")\n",
    "        self.data_cube = raw_data_package[\"data\"].astype(np.float32)\n",
    "        truth = sio.loadmat(r\"data/Indian_pines_gt.mat\")\n",
    "        self.g_truth = truth[\"groundT\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    random.seed(seed)  # Python的随机性\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)  # numpy的随机性\n",
    "    torch.manual_seed(seed)  # torch的CPU随机性，为CPU设置随机种子\n",
    "    torch.cuda.manual_seed(seed)  # torch的GPU随机性，为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "    torch.backends.cudnn.deterministic = True # 选择确定性算法\n",
    "    torch.backends.cudnn.benchmark = False # if benchmark=True, deterministic will be False\n",
    "def evaluate_performance(network_output, train_samples_gt, train_samples_gt_onehot, zeros):\n",
    "    with torch.no_grad():\n",
    "        available_label_idx = (train_samples_gt!=0).float()        # 有效标签的坐标,用于排除背景\n",
    "        available_label_count = available_label_idx.sum()          # 有效标签的个数\n",
    "        correct_prediction = torch.where(network_output==torch.argmax(train_samples_gt_onehot, 1), available_label_idx, zeros).sum()\n",
    "        OA= correct_prediction.cpu() / available_label_count\n",
    "        return OA\n",
    "def get_patch(img_size,data,data_gt,overlap_size):\n",
    "    input_size=(img_size, img_size)\n",
    "    height_orgin, width_orgin, bands = data.shape\n",
    "    image_size=(height_orgin,width_orgin)\n",
    "\n",
    "    LyEnd,LxEnd = np.subtract(image_size, input_size)\n",
    "    Lx = np.linspace(0, LxEnd, int(np.ceil(LxEnd/float(input_size[1]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "    Ly = np.linspace(0, LyEnd, int(np.ceil(LyEnd/float(input_size[0]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "    N=len(Ly)*len(Lx)\n",
    "    X_data=np.zeros([N,input_size[0],input_size[1],data.shape[-1]])#N,H,W,C\n",
    "    y_data=np.zeros([N,input_size[0],input_size[1]])\n",
    "    i=0\n",
    "    for j in range(len(Ly)):\n",
    "        for k in range(len(Lx)):\n",
    "            rStart,cStart = (Ly[j],Lx[k])\n",
    "            rEnd,cEnd = (rStart+input_size[0],cStart+input_size[1])\n",
    "            X_data[i] = data[rStart:rEnd,cStart:cEnd,:]\n",
    "            y_data[i] = data_gt[rStart:rEnd,cStart:cEnd]\n",
    "            i+=1\n",
    "    return X_data,y_data\n",
    "def Get_train_and_test_data(img_size, img,img_gt):\n",
    "    H0, W0, C = img.shape\n",
    "    if H0<img_size:\n",
    "        gap = img_size-H0\n",
    "        mirror_img = img[(H0-gap):H0,:,:]\n",
    "        mirror_img_gt = img_gt[(H0-gap):H0,:]\n",
    "        img = np.concatenate([img,mirror_img],axis=0)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=0)\n",
    "    if W0<img_size:\n",
    "        gap = img_size-W0\n",
    "        mirror_img = img[:,(W0 - gap):W0,:]\n",
    "        mirror_img_gt = img_gt[(W0-gap):W0,:]\n",
    "        img = np.concatenate([img,mirror_img],axis=1)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=1)\n",
    "    H, W, C = img.shape\n",
    "\n",
    "    num_H = H // img_size\n",
    "    num_W = W // img_size\n",
    "    sub_H = H % img_size\n",
    "    sub_W = W % img_size\n",
    "    if sub_H != 0:\n",
    "        gap = (num_H+1)*img_size - H\n",
    "        mirror_img = img[(H - gap):H, :, :]\n",
    "        mirror_img_gt = img_gt[(H - gap):H, :]\n",
    "        img = np.concatenate([img, mirror_img], axis=0)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=0)\n",
    "\n",
    "    if sub_W != 0:\n",
    "        gap = (num_W + 1) * img_size - W\n",
    "        mirror_img = img[:, (W - gap):W, :]\n",
    "        mirror_img_gt = img_gt[:, (W - gap):W]\n",
    "        img = np.concatenate([img, mirror_img], axis=1)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=1)\n",
    "        # gap = img_size - num_W*img_size\n",
    "        # img = img[:,(W - gap):W,:]\n",
    "    H, W, C = img.shape\n",
    "    print('padding img:', img.shape)\n",
    "\n",
    "    num_H = H // img_size\n",
    "    num_W = W // img_size\n",
    "    index = torch.arange(1, H*W+1)\n",
    "    index=index.reshape(H,W)\n",
    "    sub_imgs = []\n",
    "    sub_indexs= []\n",
    "\n",
    "    for i in range(num_H):\n",
    "        for j in range(num_W):\n",
    "            z = img[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :]\n",
    "            sub_imgs.append(z)\n",
    "            w = index[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size]\n",
    "            sub_indexs.append(w)\n",
    "    sub_imgs = np.array(sub_imgs)\n",
    "    sub_indexs = np.array(sub_indexs)  # [num_H*num_W,img_size,img_size, C ]\n",
    "\n",
    "    return sub_imgs,sub_indexs, num_H, num_W,img,img_gt\n",
    "def patch_reshape(pred,num_H, num_W, class_num, img_size):\n",
    "    pred = torch.reshape(pred, [num_H, num_W, class_num, img_size, img_size])\n",
    "    pred = torch.permute(pred, [2, 0, 3, 1, 4])  # [2,num_H, img_size,num_W, img_size]]\n",
    "    pred = torch.reshape(pred, [class_num, num_H * img_size* num_W * img_size])\n",
    "    pred = torch.permute(pred, [1, 0]) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = IndianRaw().normal_cube\n",
    "    data_gt = IndianRaw().truth\n",
    "    return data, data_gt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_gt = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 数据集参数配置（PU / IP / HH / HC） ============\n",
    "CONFIGS = {\n",
    "    \"PU\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 20,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 20,\n",
    "        \"val_num\": 20,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    },\n",
    "\n",
    "    \"IP\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 145,\n",
    "        \"pca_components\": 20,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 10,\n",
    "        \"val_num\": 5,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 1,\n",
    "        \"overlap_size\": 0\n",
    "    },\n",
    "\n",
    "    \"HH\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 30,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 50,\n",
    "        \"val_num\": 50,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    },\n",
    "\n",
    "    \"HC\": { \n",
    "       \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 30,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 50,\n",
    "        \"val_num\": 50,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前数据集: PU\n",
      "{'patch_size': 8, 'img_size': 128, 'pca_components': 20, 'split_type': 'number', 'train_num': 20, 'val_num': 20, 'train_ratio': 0.05, 'val_ratio': 0.01, 'max_epoch': 500, 'batch_size': 2, 'overlap_size': 32}\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"IP\"   # ← 修改这里即可切换 PU / IP / HH / HC\n",
    "\n",
    "cfg = CONFIGS[dataset_name]\n",
    "\n",
    "patch_size      = cfg[\"patch_size\"]\n",
    "img_size        = cfg[\"img_size\"]\n",
    "pca_components  = cfg[\"pca_components\"]\n",
    "split_type      = cfg[\"split_type\"]\n",
    "train_num       = cfg[\"train_num\"]\n",
    "val_num         = cfg[\"val_num\"]\n",
    "train_ratio     = cfg[\"train_ratio\"]\n",
    "val_ratio       = cfg[\"val_ratio\"]\n",
    "max_epoch       = cfg[\"max_epoch\"]\n",
    "batch_size      = cfg[\"batch_size\"]\n",
    "overlap_size    = cfg[\"overlap_size\"]\n",
    "\n",
    "print(\"当前数据集:\", dataset_name)\n",
    "print(cfg)\n",
    "\n",
    "path_weight = r\"weights//\"\n",
    "path_result = r\"result//\"\n",
    "height_orgin, width_orgin, bands = data.shape\n",
    "class_num_level2 = np.max(data_gt)\n",
    "class_num_level2 = class_num_level2.astype(int)\n",
    "setup_seed(3704)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = split_data.apply_PCA(data, num_components=pca_components)\n",
    "height_orgin, width_orgin, bands = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)\n",
    "train_index, val_index, test_index = split_data.split_data(gt_reshape, \n",
    "            class_num, train_ratio, train_ratio, train_num, val_num, split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=train_index.astype(int)\n",
    "val_index=val_index.astype(int)\n",
    "test_index=test_index.astype(int)\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_gt, test_samples_gt, val_samples_gt = create_graph.get_label(gt_reshape,\n",
    "                                                train_index, val_index, test_index)\n",
    "\n",
    "train_label_mask, test_label_mask, val_label_mask = create_graph.get_label_mask(train_samples_gt, \n",
    "                                        test_samples_gt, val_samples_gt, data_gt, class_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt = np.reshape(train_samples_gt,[height_orgin,width_orgin])\n",
    "test_gt = np.reshape(test_samples_gt,[height_orgin,width_orgin])\n",
    "val_gt = np.reshape(val_samples_gt,[height_orgin,width_orgin])\n",
    "\n",
    "\n",
    "train_gt_onehot = create_graph.label_to_one_hot(train_gt, class_num)\n",
    "test_gt_onehot = create_graph.label_to_one_hot(test_gt, class_num)\n",
    "val_gt_onehot = create_graph.label_to_one_hot(val_gt, class_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_gt=torch.from_numpy(train_samples_gt.astype(np.float32))\n",
    "test_samples_gt=torch.from_numpy(test_samples_gt.astype(np.float32))\n",
    "val_samples_gt=torch.from_numpy(val_samples_gt.astype(np.float32))\n",
    "\n",
    "train_gt_onehot = torch.from_numpy(train_gt_onehot.astype(np.float32))\n",
    "test_gt_onehot = torch.from_numpy(test_gt_onehot.astype(np.float32))\n",
    "val_gt_onehot = torch.from_numpy(val_gt_onehot.astype(np.float32))\n",
    "\n",
    "train_label_mask = torch.from_numpy(train_label_mask.astype(np.float32))\n",
    "test_label_mask = torch.from_numpy(test_label_mask.astype(np.float32))\n",
    "val_label_mask = torch.from_numpy(val_label_mask.astype(np.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_train_hsi,gt_train=get_patch(img_size, data,train_gt,overlap_size) \n",
    "img_val_hsi,gt_val=get_patch(img_size, data,val_gt,overlap_size) \n",
    "img_test_hsi,gt_test=get_patch(img_size, data,test_gt,overlap_size)  \n",
    "\n",
    "\n",
    "gt_train = torch.from_numpy(gt_train).type(torch.LongTensor) \n",
    "gt_test = torch.from_numpy(gt_test).type(torch.LongTensor)  \n",
    "gt_val = torch.from_numpy(gt_val).type(torch.LongTensor) \n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "img_train_hsi = torch.from_numpy(img_train_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n",
    "img_test_hsi = torch.from_numpy(img_test_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n",
    "img_val_hsi = torch.from_numpy(img_val_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(img_size, img_size)\n",
    "height_orgin, width_orgin, bands = data.shape\n",
    "image_size=(height_orgin,width_orgin)\n",
    "\n",
    "LyEnd,LxEnd = np.subtract(image_size, input_size)\n",
    "Lx = np.linspace(0, LxEnd, int(np.ceil(LxEnd/float(input_size[1]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "Ly = np.linspace(0, LyEnd, int(np.ceil(LyEnd/float(input_size[0]-overlap_size)))+1, endpoint=True).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_train_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_train_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_train)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Val dataset\"\"\"\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_val_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_val_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_val)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "\n",
    "\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_test_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_test_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_test) \n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "valset = ValDS()\n",
    "testset = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
    "Val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_gt=test_samples_gt.to(device)\n",
    "test_gt_onehot=test_gt_onehot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 12, 'layer_decay_rate': 0.9}\n",
      "Build LayerDecayOptimizerConstructor 0.900000 - 14\n",
      "Param groups = {\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"spat_encoder.pos_embed\",\n",
      "      \"spat_encoder.classifier.0.bias\",\n",
      "      \"spat_encoder.classifier.1.bias\",\n",
      "      \"spat_encoder.classifier.2.bias\",\n",
      "      \"spat_encoder.patch_embed.proj.bias\",\n",
      "      \"spat_encoder.blocks.0.norm1.weight\",\n",
      "      \"spat_encoder.blocks.0.norm1.bias\",\n",
      "      \"spat_encoder.blocks.0.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.0.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.0.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.0.norm2.weight\",\n",
      "      \"spat_encoder.blocks.0.norm2.bias\",\n",
      "      \"spat_encoder.blocks.0.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.0.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.1.norm1.weight\",\n",
      "      \"spat_encoder.blocks.1.norm1.bias\",\n",
      "      \"spat_encoder.blocks.1.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.1.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.1.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.1.norm2.weight\",\n",
      "      \"spat_encoder.blocks.1.norm2.bias\",\n",
      "      \"spat_encoder.blocks.1.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.1.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.2.norm1.weight\",\n",
      "      \"spat_encoder.blocks.2.norm1.bias\",\n",
      "      \"spat_encoder.blocks.2.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.2.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.2.norm2.weight\",\n",
      "      \"spat_encoder.blocks.2.norm2.bias\",\n",
      "      \"spat_encoder.blocks.2.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.2.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.3.norm1.weight\",\n",
      "      \"spat_encoder.blocks.3.norm1.bias\",\n",
      "      \"spat_encoder.blocks.3.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.3.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.3.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.3.norm2.weight\",\n",
      "      \"spat_encoder.blocks.3.norm2.bias\",\n",
      "      \"spat_encoder.blocks.3.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.3.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.4.norm1.weight\",\n",
      "      \"spat_encoder.blocks.4.norm1.bias\",\n",
      "      \"spat_encoder.blocks.4.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.4.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.4.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.4.norm2.weight\",\n",
      "      \"spat_encoder.blocks.4.norm2.bias\",\n",
      "      \"spat_encoder.blocks.4.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.4.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.5.norm1.weight\",\n",
      "      \"spat_encoder.blocks.5.norm1.bias\",\n",
      "      \"spat_encoder.blocks.5.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.5.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.5.norm2.weight\",\n",
      "      \"spat_encoder.blocks.5.norm2.bias\",\n",
      "      \"spat_encoder.blocks.5.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.5.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.6.norm1.weight\",\n",
      "      \"spat_encoder.blocks.6.norm1.bias\",\n",
      "      \"spat_encoder.blocks.6.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.6.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.6.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.6.norm2.weight\",\n",
      "      \"spat_encoder.blocks.6.norm2.bias\",\n",
      "      \"spat_encoder.blocks.6.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.6.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.7.norm1.weight\",\n",
      "      \"spat_encoder.blocks.7.norm1.bias\",\n",
      "      \"spat_encoder.blocks.7.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.7.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.7.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.7.norm2.weight\",\n",
      "      \"spat_encoder.blocks.7.norm2.bias\",\n",
      "      \"spat_encoder.blocks.7.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.7.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.8.norm1.weight\",\n",
      "      \"spat_encoder.blocks.8.norm1.bias\",\n",
      "      \"spat_encoder.blocks.8.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.8.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.8.norm2.weight\",\n",
      "      \"spat_encoder.blocks.8.norm2.bias\",\n",
      "      \"spat_encoder.blocks.8.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.8.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.9.norm1.weight\",\n",
      "      \"spat_encoder.blocks.9.norm1.bias\",\n",
      "      \"spat_encoder.blocks.9.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.9.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.9.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.9.norm2.weight\",\n",
      "      \"spat_encoder.blocks.9.norm2.bias\",\n",
      "      \"spat_encoder.blocks.9.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.9.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.10.norm1.weight\",\n",
      "      \"spat_encoder.blocks.10.norm1.bias\",\n",
      "      \"spat_encoder.blocks.10.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.10.attn.sampling_offsets.bias\",\n",
      "      \"spat_encoder.blocks.10.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.10.norm2.weight\",\n",
      "      \"spat_encoder.blocks.10.norm2.bias\",\n",
      "      \"spat_encoder.blocks.10.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.10.mlp.fc2.bias\",\n",
      "      \"spat_encoder.blocks.11.norm1.weight\",\n",
      "      \"spat_encoder.blocks.11.norm1.bias\",\n",
      "      \"spat_encoder.blocks.11.attn.qkv.bias\",\n",
      "      \"spat_encoder.blocks.11.attn.proj.bias\",\n",
      "      \"spat_encoder.blocks.11.norm2.weight\",\n",
      "      \"spat_encoder.blocks.11.norm2.bias\",\n",
      "      \"spat_encoder.blocks.11.mlp.fc1.bias\",\n",
      "      \"spat_encoder.blocks.11.mlp.fc2.bias\",\n",
      "      \"spat_encoder.norm.weight\",\n",
      "      \"spat_encoder.norm.bias\",\n",
      "      \"spec_encoder.pos_embed\",\n",
      "      \"spec_encoder.spat_map.bias\",\n",
      "      \"spec_encoder.blocks.0.norm1.weight\",\n",
      "      \"spec_encoder.blocks.0.norm1.bias\",\n",
      "      \"spec_encoder.blocks.0.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.0.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.0.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.0.norm2.weight\",\n",
      "      \"spec_encoder.blocks.0.norm2.bias\",\n",
      "      \"spec_encoder.blocks.0.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.0.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.1.norm1.weight\",\n",
      "      \"spec_encoder.blocks.1.norm1.bias\",\n",
      "      \"spec_encoder.blocks.1.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.1.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.1.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.1.norm2.weight\",\n",
      "      \"spec_encoder.blocks.1.norm2.bias\",\n",
      "      \"spec_encoder.blocks.1.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.1.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.2.norm1.weight\",\n",
      "      \"spec_encoder.blocks.2.norm1.bias\",\n",
      "      \"spec_encoder.blocks.2.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.2.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.2.norm2.weight\",\n",
      "      \"spec_encoder.blocks.2.norm2.bias\",\n",
      "      \"spec_encoder.blocks.2.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.2.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.3.norm1.weight\",\n",
      "      \"spec_encoder.blocks.3.norm1.bias\",\n",
      "      \"spec_encoder.blocks.3.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.3.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.3.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.3.norm2.weight\",\n",
      "      \"spec_encoder.blocks.3.norm2.bias\",\n",
      "      \"spec_encoder.blocks.3.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.3.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.4.norm1.weight\",\n",
      "      \"spec_encoder.blocks.4.norm1.bias\",\n",
      "      \"spec_encoder.blocks.4.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.4.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.4.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.4.norm2.weight\",\n",
      "      \"spec_encoder.blocks.4.norm2.bias\",\n",
      "      \"spec_encoder.blocks.4.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.4.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.5.norm1.weight\",\n",
      "      \"spec_encoder.blocks.5.norm1.bias\",\n",
      "      \"spec_encoder.blocks.5.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.5.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.5.norm2.weight\",\n",
      "      \"spec_encoder.blocks.5.norm2.bias\",\n",
      "      \"spec_encoder.blocks.5.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.5.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.6.norm1.weight\",\n",
      "      \"spec_encoder.blocks.6.norm1.bias\",\n",
      "      \"spec_encoder.blocks.6.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.6.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.6.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.6.norm2.weight\",\n",
      "      \"spec_encoder.blocks.6.norm2.bias\",\n",
      "      \"spec_encoder.blocks.6.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.6.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.7.norm1.weight\",\n",
      "      \"spec_encoder.blocks.7.norm1.bias\",\n",
      "      \"spec_encoder.blocks.7.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.7.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.7.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.7.norm2.weight\",\n",
      "      \"spec_encoder.blocks.7.norm2.bias\",\n",
      "      \"spec_encoder.blocks.7.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.7.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.8.norm1.weight\",\n",
      "      \"spec_encoder.blocks.8.norm1.bias\",\n",
      "      \"spec_encoder.blocks.8.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.8.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.8.norm2.weight\",\n",
      "      \"spec_encoder.blocks.8.norm2.bias\",\n",
      "      \"spec_encoder.blocks.8.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.8.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.9.norm1.weight\",\n",
      "      \"spec_encoder.blocks.9.norm1.bias\",\n",
      "      \"spec_encoder.blocks.9.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.9.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.9.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.9.norm2.weight\",\n",
      "      \"spec_encoder.blocks.9.norm2.bias\",\n",
      "      \"spec_encoder.blocks.9.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.9.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.10.norm1.weight\",\n",
      "      \"spec_encoder.blocks.10.norm1.bias\",\n",
      "      \"spec_encoder.blocks.10.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.10.attn.sampling_offsets.bias\",\n",
      "      \"spec_encoder.blocks.10.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.10.norm2.weight\",\n",
      "      \"spec_encoder.blocks.10.norm2.bias\",\n",
      "      \"spec_encoder.blocks.10.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.10.mlp.fc2.bias\",\n",
      "      \"spec_encoder.blocks.11.norm1.weight\",\n",
      "      \"spec_encoder.blocks.11.norm1.bias\",\n",
      "      \"spec_encoder.blocks.11.attn.qkv.bias\",\n",
      "      \"spec_encoder.blocks.11.attn.proj.bias\",\n",
      "      \"spec_encoder.blocks.11.norm2.weight\",\n",
      "      \"spec_encoder.blocks.11.norm2.bias\",\n",
      "      \"spec_encoder.blocks.11.mlp.fc1.bias\",\n",
      "      \"spec_encoder.blocks.11.mlp.fc2.bias\",\n",
      "      \"spec_encoder.norm.weight\",\n",
      "      \"spec_encoder.norm.bias\",\n",
      "      \"spec_encoder.conv_q.0.bias\",\n",
      "      \"spec_encoder.conv_k.0.bias\",\n",
      "      \"spec_encoder.conv_v.0.bias\",\n",
      "      \"cls.bias\",\n",
      "      \"conv.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  },\n",
      "  \"layer_13_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"spat_encoder.classifier.0.weight\",\n",
      "      \"spat_encoder.classifier.1.weight\",\n",
      "      \"spat_encoder.classifier.2.weight\",\n",
      "      \"spat_encoder.patch_embed.proj.weight\",\n",
      "      \"spat_encoder.blocks.0.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.0.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.0.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.0.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.0.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.1.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.1.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.1.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.1.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.1.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.2.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.2.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.2.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.2.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.3.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.3.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.3.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.3.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.3.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.4.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.4.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.4.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.4.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.4.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.5.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.5.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.5.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.5.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.6.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.6.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.6.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.6.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.6.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.7.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.7.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.7.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.7.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.7.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.8.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.8.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.8.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.8.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.9.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.9.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.9.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.9.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.9.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.10.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.10.attn.sampling_offsets.weight\",\n",
      "      \"spat_encoder.blocks.10.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.10.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.10.mlp.fc2.weight\",\n",
      "      \"spat_encoder.blocks.11.attn.qkv.weight\",\n",
      "      \"spat_encoder.blocks.11.attn.proj.weight\",\n",
      "      \"spat_encoder.blocks.11.mlp.fc1.weight\",\n",
      "      \"spat_encoder.blocks.11.mlp.fc2.weight\",\n",
      "      \"spec_encoder.spat_map.weight\",\n",
      "      \"spec_encoder.blocks.0.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.0.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.0.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.0.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.0.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.1.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.1.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.1.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.1.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.1.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.2.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.2.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.2.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.2.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.3.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.3.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.3.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.3.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.3.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.4.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.4.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.4.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.4.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.4.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.5.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.5.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.5.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.5.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.6.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.6.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.6.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.6.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.6.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.7.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.7.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.7.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.7.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.7.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.8.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.8.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.8.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.8.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.9.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.9.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.9.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.9.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.9.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.10.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.10.attn.sampling_offsets.weight\",\n",
      "      \"spec_encoder.blocks.10.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.10.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.10.mlp.fc2.weight\",\n",
      "      \"spec_encoder.blocks.11.attn.qkv.weight\",\n",
      "      \"spec_encoder.blocks.11.attn.proj.weight\",\n",
      "      \"spec_encoder.blocks.11.mlp.fc1.weight\",\n",
      "      \"spec_encoder.blocks.11.mlp.fc2.weight\",\n",
      "      \"spec_encoder.conv_q.0.weight\",\n",
      "      \"spec_encoder.conv_k.0.weight\",\n",
      "      \"spec_encoder.conv_v.0.weight\",\n",
      "      \"spec_encoder.l1.weight\",\n",
      "      \"conv_features.weight\",\n",
      "      \"DR1.weight\",\n",
      "      \"DR2.weight\",\n",
      "      \"DR3.weight\",\n",
      "      \"DR4.weight\",\n",
      "      \"cls.weight\",\n",
      "      \"conv.weight\",\n",
      "      \"fc_spec1.0.weight\",\n",
      "      \"fc_spec1.2.weight\",\n",
      "      \"fc_spec2.0.weight\",\n",
      "      \"fc_spec2.2.weight\",\n",
      "      \"fc_spec3.0.weight\",\n",
      "      \"fc_spec3.2.weight\",\n",
      "      \"fc_spec4.0.weight\",\n",
      "      \"fc_spec4.2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros([height_orgin * width_orgin]).to(device).float()\n",
    "model = ss_fusion_seg.SSFusionFramework(\n",
    "                img_size = img_size,\n",
    "                in_channels = pca_components,\n",
    "                patch_size=patch_size,\n",
    "                classes = class_num,\n",
    "                model_size='base'#The optional values are 'base','large' and 'huge'\n",
    ")\n",
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "    type='AdamW', lr=6e-5, betas=(0.9, 0.999), weight_decay=0.05),\n",
    "    constructor='LayerDecayOptimizerConstructor_ViT', \n",
    "    paramwise_cfg=dict(\n",
    "        num_layers=12, \n",
    "        layer_decay_rate=0.9,\n",
    "        )\n",
    "        )\n",
    "optimizer = build_optim_wrapper(model, optim_wrapper)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.optimizer, max_epoch, eta_min=0, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "count = 0\n",
    "best_loss = 99999\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_index =train_index.reshape(-1,)\n",
    "test_index =test_index.reshape(-1,)\n",
    "val_loss = 0\n",
    "train_loss = 0\n",
    "train_correct = 0\n",
    "train_total= 1\n",
    "val_correct = 0\n",
    "val_total= 0\n",
    "best_loss=999\n",
    "test_correct=0\n",
    "test_total=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params =model.state_dict()\n",
    "spat_net = torch.load((r\"spat-base.pth\"), map_location=torch.device('cpu'))\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_map' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_output_maps' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'pos_embed' in k:\n",
    "        del spat_net['model'][k]\n",
    "spat_weights = {}\n",
    "prefix = 'spat_encoder.'\n",
    "for key, value in spat_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spat_weights[new_key] = value\n",
    "per_net = torch.load((r\"spec-base.pth\"), map_location=torch.device('cpu'))\n",
    "model_params =model.state_dict()\n",
    "for k in list(per_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'spat_map' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'fpn1.0.weight' in k:\n",
    "        del per_net['model'][k]\n",
    "spec_weights = {}\n",
    "prefix = 'spec_encoder.'\n",
    "for key, value in per_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spec_weights[new_key] = value\n",
    "model_params =model.state_dict()\n",
    "for k in list(spec_weights.keys()):\n",
    "    if 'spec_encoder.patch_embed' in k:\n",
    "        del spec_weights[k]\n",
    "merged_params = {**spat_weights, **spec_weights}\n",
    "same_parsms = {k: v for k, v in merged_params.items() if k in model_params.keys()}\n",
    "model_params.update(same_parsms)\n",
    "model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hsi=torch.from_numpy(data).permute(2,0,1).to(device)\n",
    "img_hsi = img_hsi.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "OA_train 0.1650943396226415\n",
      "loss_train 51.218028485774994\n",
      "OA_val 0.29603729603729606\n",
      "loss_val 26.367133021354675\n",
      "######################save model######################\n",
      "epoch 10\n",
      "OA_train 0.8158573270305114\n",
      "loss_train 0.7228915365412831\n",
      "OA_val 0.5990675990675991\n",
      "loss_val 4.920184265822172\n",
      "######################save model######################\n",
      "epoch 20\n",
      "OA_train 0.9010580819450698\n",
      "loss_train 0.08274528034962714\n",
      "OA_val 0.6985236985236986\n",
      "loss_val 3.8707724497653544\n",
      "######################save model######################\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(100+ 1):\n",
    "    count=0\n",
    "    train_loss = 0\n",
    "    for netinput_hsi,netinput_gt in train_loader:\n",
    "        model.train()\n",
    "        netinput_hsi = netinput_hsi.to(device)\n",
    "        b,c,h,w = netinput_hsi.shape\n",
    "        batch_pred = model(netinput_hsi)\n",
    "        batch_pred = batch_pred.reshape(b*img_size*img_size,-1)\n",
    "        netinput_gt = netinput_gt.reshape(-1)\n",
    "        available_label_idx = np.where(netinput_gt!=0)\n",
    "        if len(available_label_idx[0]) != 0:\n",
    "            available_label_idx= torch.from_numpy(available_label_idx[0]).to(device)\n",
    "            netinput_gt = netinput_gt.to(device)\n",
    "            netinput_gt = netinput_gt-1\n",
    "            batch_pred_loss = batch_pred[available_label_idx]\n",
    "            netinput_gt_loss = netinput_gt[available_label_idx]\n",
    "            loss = criterion(batch_pred_loss, netinput_gt_loss.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss+loss.item()\n",
    "            y_pred = torch.argmax(batch_pred_loss, dim=1)\n",
    "            train_correct += (y_pred == netinput_gt_loss).sum().item()\n",
    "            train_total += netinput_gt_loss.size(0)\n",
    "            if count == 0:\n",
    "                y_pred_train =  y_pred.cpu().numpy()\n",
    "                y_gt_train = netinput_gt_loss.cpu().numpy()\n",
    "                count = 1\n",
    "            else:\n",
    "                y_pred_train = np.concatenate( (y_pred_train, y_pred.cpu().numpy()) )\n",
    "                y_gt_train = np.concatenate( (y_gt_train, netinput_gt_loss.cpu().numpy()) ) \n",
    "    OA_train = train_correct / train_total\n",
    "    if epoch%10==0:\n",
    "        count=0\n",
    "        val_loss = 0\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for netinput_hsi,netinput_gt in Val_loader:\n",
    "                netinput_hsi = netinput_hsi.to(device)\n",
    "                b,c,h,w = netinput_hsi.shape\n",
    "                batch_pred = model(netinput_hsi)\n",
    "                batch_pred = batch_pred.reshape(b*img_size*img_size,-1)\n",
    "                netinput_gt = netinput_gt.reshape(-1)\n",
    "                available_label_idx = np.where(netinput_gt!=0)\n",
    "                if len(available_label_idx[0]) != 0:\n",
    "                    available_label_idx= torch.from_numpy(available_label_idx[0]).to(device)\n",
    "                    netinput_gt = netinput_gt.to(device)\n",
    "                    netinput_gt = netinput_gt-1\n",
    "                    batch_pred_loss = batch_pred[available_label_idx]\n",
    "                    netinput_gt_loss = netinput_gt[available_label_idx]\n",
    "                    loss_val = criterion(batch_pred_loss, netinput_gt_loss.long())\n",
    "                    val_loss =val_loss+loss_val.item()\n",
    "                    y_pred = torch.argmax(batch_pred_loss, dim=1)\n",
    "                    val_correct += (y_pred == netinput_gt_loss).sum().item()\n",
    "                    val_total += netinput_gt_loss.size(0)\n",
    "                    if count == 0:\n",
    "                        y_pred_val =  y_pred.cpu().numpy()\n",
    "                        y_gt_val = netinput_gt_loss.cpu().numpy()\n",
    "                        count = 1\n",
    "                    else:\n",
    "                        y_pred_val = np.concatenate( (y_pred_val, y_pred.cpu().numpy()) )\n",
    "                        y_gt_val = np.concatenate( (y_gt_val, netinput_gt_loss.cpu().numpy()) )  \n",
    "            OA_val = val_correct / val_total \n",
    "\n",
    "        print('epoch',epoch)\n",
    "        print('OA_train',OA_train)\n",
    "        print('loss_train',train_loss)\n",
    "        print('OA_val',OA_val)\n",
    "        print('loss_val',val_loss)\n",
    "\n",
    "        if val_loss < best_loss :\n",
    "            best_loss = val_loss\n",
    "            print('######################save model######################')\n",
    "            torch.save(model.state_dict(), path_weight + r\"model.pt\")\n",
    "        count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test OA=0.9242\n",
      "kappa= [0.8999523172057162]\n",
      "each_acc= [array([0.72477621, 0.98113816, 0.98737251, 0.79563492, 0.95555556,\n",
      "       0.98296252, 0.96434109, 0.98682043, 0.81477398])]\n",
      "average_acc= [0.9103750408174558]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load(path_weight + r\"model.pt\"))\n",
    "    model.eval()\n",
    "    y_tes_pred_combine = np.zeros([height_orgin,width_orgin])\n",
    "    for j in range(len(Ly)):\n",
    "        for k in range(len(Lx)):\n",
    "            rStart, cStart = (Ly[j], Lx[k])\n",
    "            rEnd, cEnd = (rStart + input_size[0], cStart + input_size[1])\n",
    "            img_part_hsi = img_hsi[:,rStart:rEnd,cStart:cEnd].unsqueeze(0)\n",
    "            batch_pred = model(img_part_hsi)\n",
    "            batch_pred = torch.squeeze(batch_pred, 0)\n",
    "            batch_pred = batch_pred.detach().cpu().numpy()\n",
    "            batch_pred = np.argmax(batch_pred,1).reshape(img_size,img_size)\n",
    "            if j == 0 and k == 0:\n",
    "                y_tes_pred_combine[rStart:rEnd, cStart:cEnd] = batch_pred\n",
    "            elif j == 0 and k > 0:\n",
    "                y_tes_pred_combine[rStart:rEnd, cStart + int(overlap_size / 2):cEnd] = batch_pred[:,\n",
    "                                                                                    int(overlap_size / 2):]\n",
    "            elif j > 0 and k == 0:\n",
    "                y_tes_pred_combine[rStart + int(overlap_size / 2):rEnd, cStart:cEnd] = batch_pred[\n",
    "                                                                                    int(overlap_size / 2):,\n",
    "                                                                                    :]\n",
    "            else:\n",
    "                y_tes_pred_combine[rStart + int(overlap_size / 2):rEnd,\n",
    "                cStart + int(overlap_size / 2):cEnd] = batch_pred[int(overlap_size / 2):,\n",
    "                                                            int(overlap_size / 2):]\n",
    "\n",
    "y_tes_pred_combine =torch.from_numpy(y_tes_pred_combine).type(torch.LongTensor).to(device)+1\n",
    "overall_acc,OA_hi1,average_acc,kappa,each_acc=utils.evaluate_performance_all(y_tes_pred_combine.reshape(height_orgin*width_orgin), test_samples_gt, test_gt_onehot,  height_orgin, width_orgin, class_num, test_gt,device, require_AA_KPP=True, printFlag=False)\n",
    "print(\"test OA={:.4f}\".format(overall_acc))\n",
    "print('kappa=',kappa)\n",
    "print('each_acc=',each_acc)\n",
    "print('average_acc=',average_acc)\n",
    "testOA_combine = evaluate_performance(y_tes_pred_combine.reshape(height_orgin*width_orgin), test_samples_gt, test_gt_onehot, zeros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
