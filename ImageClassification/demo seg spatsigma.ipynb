{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import SpatViT_seg\n",
    "import torch\n",
    "from torch  import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,cohen_kappa_score\n",
    "from model import split_data,utils,create_graph\n",
    "#from sklearn import metrics, preprocessing\n",
    "from mmengine.optim import build_optim_wrapper\n",
    "from mmcv_custom import custom_layer_decay_optimizer_constructor,layer_decay_optimizer_constructor_vit\n",
    "import torch.utils.data as Data\n",
    "import scipy.io as sio\n",
    "import spectral as spy\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA\n",
    "import random\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.data_cube = None\n",
    "        self.g_truth = None\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        \"\"\"\n",
    "        origin data\n",
    "        \"\"\"\n",
    "        return self.data_cube\n",
    "\n",
    "    @property\n",
    "    def truth(self):\n",
    "        return self.g_truth\n",
    "\n",
    "    @property\n",
    "    def normal_cube(self):\n",
    "        \"\"\"\n",
    "        normalization data: range(0, 1)\n",
    "        \"\"\"\n",
    "        return (self.data_cube - np.min(self.data_cube)) / (np.max(self.data_cube) - np.min(self.data_cube))\n",
    "class IndianRaw(DataReader):\n",
    "    def __init__(self):\n",
    "        super(IndianRaw, self).__init__()\n",
    "        raw_data_package = sio.loadmat(r\"data/Indian_pines_corrected.mat\")\n",
    "        self.data_cube = raw_data_package[\"data\"].astype(np.float32)\n",
    "        truth = sio.loadmat(r\"data/Indian_pines_gt.mat\")\n",
    "        self.g_truth = truth[\"groundT\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "    random.seed(seed)  # Python的随机性\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)  # 设置Python哈希种子，为了禁止hash随机化，使得实验可复现\n",
    "    np.random.seed(seed)  # numpy的随机性\n",
    "    torch.manual_seed(seed)  # torch的CPU随机性，为CPU设置随机种子\n",
    "    torch.cuda.manual_seed(seed)  # torch的GPU随机性，为当前GPU设置随机种子\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.   torch的GPU随机性，为所有GPU设置随机种子\n",
    "    torch.backends.cudnn.deterministic = True # 选择确定性算法\n",
    "    torch.backends.cudnn.benchmark = False # if benchmark=True, deterministic will be False\n",
    "def evaluate_performance(network_output, train_samples_gt, train_samples_gt_onehot, zeros):\n",
    "    with torch.no_grad():\n",
    "        available_label_idx = (train_samples_gt!=0).float()        # 有效标签的坐标,用于排除背景\n",
    "        available_label_count = available_label_idx.sum()          # 有效标签的个数\n",
    "        correct_prediction = torch.where(network_output==torch.argmax(train_samples_gt_onehot, 1), available_label_idx, zeros).sum()\n",
    "        OA= correct_prediction.cpu() / available_label_count\n",
    "        return OA\n",
    "def get_patch(img_size,data,data_gt,overlap_size):\n",
    "    input_size=(img_size, img_size)\n",
    "    height_orgin, width_orgin, bands = data.shape\n",
    "    image_size=(height_orgin,width_orgin)\n",
    "\n",
    "    LyEnd,LxEnd = np.subtract(image_size, input_size)\n",
    "    Lx = np.linspace(0, LxEnd, int(np.ceil(LxEnd/float(input_size[1]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "    Ly = np.linspace(0, LyEnd, int(np.ceil(LyEnd/float(input_size[0]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "    N=len(Ly)*len(Lx)\n",
    "    X_data=np.zeros([N,input_size[0],input_size[1],data.shape[-1]])#N,H,W,C\n",
    "    y_data=np.zeros([N,input_size[0],input_size[1]])\n",
    "    i=0\n",
    "    for j in range(len(Ly)):\n",
    "        for k in range(len(Lx)):\n",
    "            rStart,cStart = (Ly[j],Lx[k])\n",
    "            rEnd,cEnd = (rStart+input_size[0],cStart+input_size[1])\n",
    "            X_data[i] = data[rStart:rEnd,cStart:cEnd,:]\n",
    "            y_data[i] = data_gt[rStart:rEnd,cStart:cEnd]\n",
    "            i+=1\n",
    "    return X_data,y_data\n",
    "def Get_train_and_test_data(img_size, img,img_gt):\n",
    "    H0, W0, C = img.shape\n",
    "    if H0<img_size:\n",
    "        gap = img_size-H0\n",
    "        mirror_img = img[(H0-gap):H0,:,:]\n",
    "        mirror_img_gt = img_gt[(H0-gap):H0,:]\n",
    "        img = np.concatenate([img,mirror_img],axis=0)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=0)\n",
    "    if W0<img_size:\n",
    "        gap = img_size-W0\n",
    "        mirror_img = img[:,(W0 - gap):W0,:]\n",
    "        mirror_img_gt = img_gt[(W0-gap):W0,:]\n",
    "        img = np.concatenate([img,mirror_img],axis=1)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=1)\n",
    "    H, W, C = img.shape\n",
    "\n",
    "    num_H = H // img_size\n",
    "    num_W = W // img_size\n",
    "    sub_H = H % img_size\n",
    "    sub_W = W % img_size\n",
    "    if sub_H != 0:\n",
    "        gap = (num_H+1)*img_size - H\n",
    "        mirror_img = img[(H - gap):H, :, :]\n",
    "        mirror_img_gt = img_gt[(H - gap):H, :]\n",
    "        img = np.concatenate([img, mirror_img], axis=0)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=0)\n",
    "\n",
    "    if sub_W != 0:\n",
    "        gap = (num_W + 1) * img_size - W\n",
    "        mirror_img = img[:, (W - gap):W, :]\n",
    "        mirror_img_gt = img_gt[:, (W - gap):W]\n",
    "        img = np.concatenate([img, mirror_img], axis=1)\n",
    "        img_gt = np.concatenate([img_gt,mirror_img_gt],axis=1)\n",
    "        # gap = img_size - num_W*img_size\n",
    "        # img = img[:,(W - gap):W,:]\n",
    "    H, W, C = img.shape\n",
    "    print('padding img:', img.shape)\n",
    "\n",
    "    num_H = H // img_size\n",
    "    num_W = W // img_size\n",
    "    index = torch.arange(1, H*W+1)\n",
    "    index=index.reshape(H,W)\n",
    "    sub_imgs = []\n",
    "    sub_indexs= []\n",
    "\n",
    "    for i in range(num_H):\n",
    "        for j in range(num_W):\n",
    "            z = img[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size, :]\n",
    "            sub_imgs.append(z)\n",
    "            w = index[i * img_size:(i + 1) * img_size, j * img_size:(j + 1) * img_size]\n",
    "            sub_indexs.append(w)\n",
    "    sub_imgs = np.array(sub_imgs)\n",
    "    sub_indexs = np.array(sub_indexs)  # [num_H*num_W,img_size,img_size, C ]\n",
    "\n",
    "    return sub_imgs,sub_indexs, num_H, num_W,img,img_gt\n",
    "def patch_reshape(pred,num_H, num_W, class_num, img_size):\n",
    "    pred = torch.reshape(pred, [num_H, num_W, class_num, img_size, img_size])\n",
    "    pred = torch.permute(pred, [2, 0, 3, 1, 4])  # [2,num_H, img_size,num_W, img_size]]\n",
    "    pred = torch.reshape(pred, [class_num, num_H * img_size* num_W * img_size])\n",
    "    pred = torch.permute(pred, [1, 0]) \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = IndianRaw().normal_cube\n",
    "    data_gt = IndianRaw().truth\n",
    "    return data, data_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, data_gt = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============ 数据集参数配置（PU / IP / HH / HC） ============\n",
    "CONFIGS = {\n",
    "    \"PU\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 20,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 20,\n",
    "        \"val_num\": 20,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    },\n",
    "\n",
    "    \"IP\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 145,\n",
    "        \"pca_components\": 20,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 10,\n",
    "        \"val_num\": 5,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 1,\n",
    "        \"overlap_size\": 0\n",
    "    },\n",
    "\n",
    "    \"HH\": {  \n",
    "        \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 30,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 50,\n",
    "        \"val_num\": 50,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    },\n",
    "\n",
    "    \"HC\": { \n",
    "       \"patch_size\": 8,\n",
    "        \"img_size\": 128,\n",
    "        \"pca_components\": 30,\n",
    "        \"split_type\": \"number\",\n",
    "        \"train_num\": 50,\n",
    "        \"val_num\": 50,\n",
    "        \"train_ratio\": 0.05,\n",
    "        \"val_ratio\": 0.01,\n",
    "        \"max_epoch\": 500,\n",
    "        \"batch_size\": 2,\n",
    "        \"overlap_size\": 32\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"IP\"   # ← 修改这里即可切换 PU / IP / HH / HC\n",
    "\n",
    "cfg = CONFIGS[dataset_name]\n",
    "\n",
    "patch_size      = cfg[\"patch_size\"]\n",
    "img_size        = cfg[\"img_size\"]\n",
    "pca_components  = cfg[\"pca_components\"]\n",
    "split_type      = cfg[\"split_type\"]\n",
    "train_num       = cfg[\"train_num\"]\n",
    "val_num         = cfg[\"val_num\"]\n",
    "train_ratio     = cfg[\"train_ratio\"]\n",
    "val_ratio       = cfg[\"val_ratio\"]\n",
    "max_epoch       = cfg[\"max_epoch\"]\n",
    "batch_size      = cfg[\"batch_size\"]\n",
    "overlap_size    = cfg[\"overlap_size\"]\n",
    "\n",
    "print(\"当前数据集:\", dataset_name)\n",
    "print(cfg)\n",
    "\n",
    "path_weight = r\"weights//\"\n",
    "path_result = r\"result//\"\n",
    "height_orgin, width_orgin, bands = data.shape\n",
    "class_num_level2 = np.max(data_gt)\n",
    "class_num_level2 = class_num_level2.astype(int)\n",
    "setup_seed(3704)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = split_data.apply_PCA(data, num_components=pca_components)\n",
    "height_orgin, width_orgin, bands = data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_reshape = np.reshape(data_gt, [-1])\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)\n",
    "train_index, val_index, test_index = split_data.split_data(gt_reshape, \n",
    "            class_num, train_ratio, train_ratio, train_num, val_num, split_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index=train_index.astype(int)\n",
    "val_index=val_index.astype(int)\n",
    "test_index=test_index.astype(int)\n",
    "class_num = np.max(gt_reshape)\n",
    "class_num = class_num.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_gt, test_samples_gt, val_samples_gt = create_graph.get_label(gt_reshape,\n",
    "                                                train_index, val_index, test_index)\n",
    "\n",
    "train_label_mask, test_label_mask, val_label_mask = create_graph.get_label_mask(train_samples_gt, \n",
    "                                        test_samples_gt, val_samples_gt, data_gt, class_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gt = np.reshape(train_samples_gt,[height_orgin,width_orgin])\n",
    "test_gt = np.reshape(test_samples_gt,[height_orgin,width_orgin])\n",
    "val_gt = np.reshape(val_samples_gt,[height_orgin,width_orgin])\n",
    "\n",
    "\n",
    "train_gt_onehot = create_graph.label_to_one_hot(train_gt, class_num)\n",
    "test_gt_onehot = create_graph.label_to_one_hot(test_gt, class_num)\n",
    "val_gt_onehot = create_graph.label_to_one_hot(val_gt, class_num)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_samples_gt=torch.from_numpy(train_samples_gt.astype(np.float32))\n",
    "test_samples_gt=torch.from_numpy(test_samples_gt.astype(np.float32))\n",
    "val_samples_gt=torch.from_numpy(val_samples_gt.astype(np.float32))\n",
    "\n",
    "train_gt_onehot = torch.from_numpy(train_gt_onehot.astype(np.float32))\n",
    "test_gt_onehot = torch.from_numpy(test_gt_onehot.astype(np.float32))\n",
    "val_gt_onehot = torch.from_numpy(val_gt_onehot.astype(np.float32))\n",
    "\n",
    "train_label_mask = torch.from_numpy(train_label_mask.astype(np.float32))\n",
    "test_label_mask = torch.from_numpy(test_label_mask.astype(np.float32))\n",
    "val_label_mask = torch.from_numpy(val_label_mask.astype(np.float32))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "img_train_hsi,gt_train=get_patch(img_size, data,train_gt,overlap_size) \n",
    "img_val_hsi,gt_val=get_patch(img_size, data,val_gt,overlap_size) \n",
    "img_test_hsi,gt_test=get_patch(img_size, data,test_gt,overlap_size)  \n",
    "\n",
    "\n",
    "gt_train = torch.from_numpy(gt_train).type(torch.LongTensor) \n",
    "gt_test = torch.from_numpy(gt_test).type(torch.LongTensor)  \n",
    "gt_val = torch.from_numpy(gt_val).type(torch.LongTensor) \n",
    "\n",
    "\n",
    "####################################################################\n",
    "\n",
    "img_train_hsi = torch.from_numpy(img_train_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n",
    "img_test_hsi = torch.from_numpy(img_test_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n",
    "img_val_hsi = torch.from_numpy(img_val_hsi.transpose(0,3,1,2)).type(torch.FloatTensor) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size=(img_size, img_size)\n",
    "height_orgin, width_orgin, bands = data.shape\n",
    "image_size=(height_orgin,width_orgin)\n",
    "\n",
    "LyEnd,LxEnd = np.subtract(image_size, input_size)\n",
    "Lx = np.linspace(0, LxEnd, int(np.ceil(LxEnd/float(input_size[1]-overlap_size)))+1, endpoint=True).astype('int')\n",
    "Ly = np.linspace(0, LyEnd, int(np.ceil(LyEnd/float(input_size[0]-overlap_size)))+1, endpoint=True).astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_train_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_train_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_train)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\"\"\" Val dataset\"\"\"\n",
    "class ValDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_val_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_val_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_val)\n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "\n",
    "\"\"\" Testing dataset\"\"\"\n",
    "\n",
    "\n",
    "class TestDS(torch.utils.data.Dataset):\n",
    "    def __init__(self):\n",
    "        self.len = img_test_hsi.shape[0]\n",
    "        self.x_data_hsi = torch.FloatTensor(img_test_hsi)\n",
    "        self.y_data = torch.LongTensor(gt_test) \n",
    "    def __getitem__(self, index):\n",
    "        # 根据索引返回数据和对应的标签\n",
    "        return self.x_data_hsi[index], self.y_data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回文件数据的数目\n",
    "        return self.len\n",
    "\n",
    "\n",
    "# 创建 trainloader 和 testloader\n",
    "trainset = TrainDS()\n",
    "valset = ValDS()\n",
    "testset = TestDS()\n",
    "train_loader = torch.utils.data.DataLoader(dataset=trainset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
    "Val_loader = torch.utils.data.DataLoader(dataset=valset, batch_size=batch_size, shuffle=True, num_workers=0,drop_last=True)\n",
    "test_loader = torch.utils.data.DataLoader(dataset=testset, batch_size=batch_size, shuffle=False, num_workers=0,drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples_gt=test_samples_gt.to(device)\n",
    "test_gt_onehot=test_gt_onehot.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_layers': 12, 'layer_decay_rate': 0.9}\n",
      "Build LayerDecayOptimizerConstructor 0.900000 - 14\n",
      "Param groups = {\n",
      "  \"layer_13_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"DR.weight\",\n",
      "      \"cls.weight\",\n",
      "      \"classifier.0.weight\",\n",
      "      \"classifier.1.weight\",\n",
      "      \"classifier.2.weight\",\n",
      "      \"patch_embed.proj.weight\",\n",
      "      \"blocks.0.attn.qkv.weight\",\n",
      "      \"blocks.0.attn.sampling_offsets.weight\",\n",
      "      \"blocks.0.attn.proj.weight\",\n",
      "      \"blocks.0.mlp.fc1.weight\",\n",
      "      \"blocks.0.mlp.fc2.weight\",\n",
      "      \"blocks.1.attn.qkv.weight\",\n",
      "      \"blocks.1.attn.sampling_offsets.weight\",\n",
      "      \"blocks.1.attn.proj.weight\",\n",
      "      \"blocks.1.mlp.fc1.weight\",\n",
      "      \"blocks.1.mlp.fc2.weight\",\n",
      "      \"blocks.2.attn.qkv.weight\",\n",
      "      \"blocks.2.attn.proj.weight\",\n",
      "      \"blocks.2.mlp.fc1.weight\",\n",
      "      \"blocks.2.mlp.fc2.weight\",\n",
      "      \"blocks.3.attn.qkv.weight\",\n",
      "      \"blocks.3.attn.sampling_offsets.weight\",\n",
      "      \"blocks.3.attn.proj.weight\",\n",
      "      \"blocks.3.mlp.fc1.weight\",\n",
      "      \"blocks.3.mlp.fc2.weight\",\n",
      "      \"blocks.4.attn.qkv.weight\",\n",
      "      \"blocks.4.attn.sampling_offsets.weight\",\n",
      "      \"blocks.4.attn.proj.weight\",\n",
      "      \"blocks.4.mlp.fc1.weight\",\n",
      "      \"blocks.4.mlp.fc2.weight\",\n",
      "      \"blocks.5.attn.qkv.weight\",\n",
      "      \"blocks.5.attn.proj.weight\",\n",
      "      \"blocks.5.mlp.fc1.weight\",\n",
      "      \"blocks.5.mlp.fc2.weight\",\n",
      "      \"blocks.6.attn.qkv.weight\",\n",
      "      \"blocks.6.attn.sampling_offsets.weight\",\n",
      "      \"blocks.6.attn.proj.weight\",\n",
      "      \"blocks.6.mlp.fc1.weight\",\n",
      "      \"blocks.6.mlp.fc2.weight\",\n",
      "      \"blocks.7.attn.qkv.weight\",\n",
      "      \"blocks.7.attn.sampling_offsets.weight\",\n",
      "      \"blocks.7.attn.proj.weight\",\n",
      "      \"blocks.7.mlp.fc1.weight\",\n",
      "      \"blocks.7.mlp.fc2.weight\",\n",
      "      \"blocks.8.attn.qkv.weight\",\n",
      "      \"blocks.8.attn.proj.weight\",\n",
      "      \"blocks.8.mlp.fc1.weight\",\n",
      "      \"blocks.8.mlp.fc2.weight\",\n",
      "      \"blocks.9.attn.qkv.weight\",\n",
      "      \"blocks.9.attn.sampling_offsets.weight\",\n",
      "      \"blocks.9.attn.proj.weight\",\n",
      "      \"blocks.9.mlp.fc1.weight\",\n",
      "      \"blocks.9.mlp.fc2.weight\",\n",
      "      \"blocks.10.attn.qkv.weight\",\n",
      "      \"blocks.10.attn.sampling_offsets.weight\",\n",
      "      \"blocks.10.attn.proj.weight\",\n",
      "      \"blocks.10.mlp.fc1.weight\",\n",
      "      \"blocks.10.mlp.fc2.weight\",\n",
      "      \"blocks.11.attn.qkv.weight\",\n",
      "      \"blocks.11.attn.proj.weight\",\n",
      "      \"blocks.11.mlp.fc1.weight\",\n",
      "      \"blocks.11.mlp.fc2.weight\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.05\n",
      "  },\n",
      "  \"layer_13_no_decay\": {\n",
      "    \"param_names\": [\n",
      "      \"cls.bias\",\n",
      "      \"classifier.0.bias\",\n",
      "      \"classifier.1.bias\",\n",
      "      \"classifier.2.bias\",\n",
      "      \"patch_embed.proj.bias\",\n",
      "      \"blocks.0.norm1.weight\",\n",
      "      \"blocks.0.norm1.bias\",\n",
      "      \"blocks.0.attn.qkv.bias\",\n",
      "      \"blocks.0.attn.sampling_offsets.bias\",\n",
      "      \"blocks.0.attn.proj.bias\",\n",
      "      \"blocks.0.norm2.weight\",\n",
      "      \"blocks.0.norm2.bias\",\n",
      "      \"blocks.0.mlp.fc1.bias\",\n",
      "      \"blocks.0.mlp.fc2.bias\",\n",
      "      \"blocks.1.norm1.weight\",\n",
      "      \"blocks.1.norm1.bias\",\n",
      "      \"blocks.1.attn.qkv.bias\",\n",
      "      \"blocks.1.attn.sampling_offsets.bias\",\n",
      "      \"blocks.1.attn.proj.bias\",\n",
      "      \"blocks.1.norm2.weight\",\n",
      "      \"blocks.1.norm2.bias\",\n",
      "      \"blocks.1.mlp.fc1.bias\",\n",
      "      \"blocks.1.mlp.fc2.bias\",\n",
      "      \"blocks.2.norm1.weight\",\n",
      "      \"blocks.2.norm1.bias\",\n",
      "      \"blocks.2.attn.qkv.bias\",\n",
      "      \"blocks.2.attn.proj.bias\",\n",
      "      \"blocks.2.norm2.weight\",\n",
      "      \"blocks.2.norm2.bias\",\n",
      "      \"blocks.2.mlp.fc1.bias\",\n",
      "      \"blocks.2.mlp.fc2.bias\",\n",
      "      \"blocks.3.norm1.weight\",\n",
      "      \"blocks.3.norm1.bias\",\n",
      "      \"blocks.3.attn.qkv.bias\",\n",
      "      \"blocks.3.attn.sampling_offsets.bias\",\n",
      "      \"blocks.3.attn.proj.bias\",\n",
      "      \"blocks.3.norm2.weight\",\n",
      "      \"blocks.3.norm2.bias\",\n",
      "      \"blocks.3.mlp.fc1.bias\",\n",
      "      \"blocks.3.mlp.fc2.bias\",\n",
      "      \"blocks.4.norm1.weight\",\n",
      "      \"blocks.4.norm1.bias\",\n",
      "      \"blocks.4.attn.qkv.bias\",\n",
      "      \"blocks.4.attn.sampling_offsets.bias\",\n",
      "      \"blocks.4.attn.proj.bias\",\n",
      "      \"blocks.4.norm2.weight\",\n",
      "      \"blocks.4.norm2.bias\",\n",
      "      \"blocks.4.mlp.fc1.bias\",\n",
      "      \"blocks.4.mlp.fc2.bias\",\n",
      "      \"blocks.5.norm1.weight\",\n",
      "      \"blocks.5.norm1.bias\",\n",
      "      \"blocks.5.attn.qkv.bias\",\n",
      "      \"blocks.5.attn.proj.bias\",\n",
      "      \"blocks.5.norm2.weight\",\n",
      "      \"blocks.5.norm2.bias\",\n",
      "      \"blocks.5.mlp.fc1.bias\",\n",
      "      \"blocks.5.mlp.fc2.bias\",\n",
      "      \"blocks.6.norm1.weight\",\n",
      "      \"blocks.6.norm1.bias\",\n",
      "      \"blocks.6.attn.qkv.bias\",\n",
      "      \"blocks.6.attn.sampling_offsets.bias\",\n",
      "      \"blocks.6.attn.proj.bias\",\n",
      "      \"blocks.6.norm2.weight\",\n",
      "      \"blocks.6.norm2.bias\",\n",
      "      \"blocks.6.mlp.fc1.bias\",\n",
      "      \"blocks.6.mlp.fc2.bias\",\n",
      "      \"blocks.7.norm1.weight\",\n",
      "      \"blocks.7.norm1.bias\",\n",
      "      \"blocks.7.attn.qkv.bias\",\n",
      "      \"blocks.7.attn.sampling_offsets.bias\",\n",
      "      \"blocks.7.attn.proj.bias\",\n",
      "      \"blocks.7.norm2.weight\",\n",
      "      \"blocks.7.norm2.bias\",\n",
      "      \"blocks.7.mlp.fc1.bias\",\n",
      "      \"blocks.7.mlp.fc2.bias\",\n",
      "      \"blocks.8.norm1.weight\",\n",
      "      \"blocks.8.norm1.bias\",\n",
      "      \"blocks.8.attn.qkv.bias\",\n",
      "      \"blocks.8.attn.proj.bias\",\n",
      "      \"blocks.8.norm2.weight\",\n",
      "      \"blocks.8.norm2.bias\",\n",
      "      \"blocks.8.mlp.fc1.bias\",\n",
      "      \"blocks.8.mlp.fc2.bias\",\n",
      "      \"blocks.9.norm1.weight\",\n",
      "      \"blocks.9.norm1.bias\",\n",
      "      \"blocks.9.attn.qkv.bias\",\n",
      "      \"blocks.9.attn.sampling_offsets.bias\",\n",
      "      \"blocks.9.attn.proj.bias\",\n",
      "      \"blocks.9.norm2.weight\",\n",
      "      \"blocks.9.norm2.bias\",\n",
      "      \"blocks.9.mlp.fc1.bias\",\n",
      "      \"blocks.9.mlp.fc2.bias\",\n",
      "      \"blocks.10.norm1.weight\",\n",
      "      \"blocks.10.norm1.bias\",\n",
      "      \"blocks.10.attn.qkv.bias\",\n",
      "      \"blocks.10.attn.sampling_offsets.bias\",\n",
      "      \"blocks.10.attn.proj.bias\",\n",
      "      \"blocks.10.norm2.weight\",\n",
      "      \"blocks.10.norm2.bias\",\n",
      "      \"blocks.10.mlp.fc1.bias\",\n",
      "      \"blocks.10.mlp.fc2.bias\",\n",
      "      \"blocks.11.norm1.weight\",\n",
      "      \"blocks.11.norm1.bias\",\n",
      "      \"blocks.11.attn.qkv.bias\",\n",
      "      \"blocks.11.attn.proj.bias\",\n",
      "      \"blocks.11.norm2.weight\",\n",
      "      \"blocks.11.norm2.bias\",\n",
      "      \"blocks.11.mlp.fc1.bias\",\n",
      "      \"blocks.11.mlp.fc2.bias\",\n",
      "      \"norm.weight\",\n",
      "      \"norm.bias\"\n",
      "    ],\n",
      "    \"lr_scale\": 1.0,\n",
      "    \"lr\": 6e-05,\n",
      "    \"weight_decay\": 0.0\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "zeros = torch.zeros([height_orgin * width_orgin]).to(device).float()\n",
    "model = SpatViT_seg.SpatViT (img_size=img_size,\n",
    "            num_classes = class_num,\n",
    "            in_chans=pca_components,\n",
    "            patch_size=patch_size,\n",
    "            drop_path_rate=0.1,\n",
    "            out_indices=[3, 5, 7, 11],\n",
    "            embed_dim=768,\n",
    "            depth=12,\n",
    "            num_heads=12,\n",
    "            mlp_ratio=4,\n",
    "            qkv_bias=True,\n",
    "            qk_scale=None,\n",
    "            drop_rate=0.,\n",
    "            attn_drop_rate=0.,\n",
    "            use_checkpoint=True,\n",
    "            use_abs_pos_emb=False,\n",
    "            interval = 3,\n",
    "            n_points=8).to(device)\n",
    "optim_wrapper = dict(\n",
    "    optimizer=dict(\n",
    "    type='AdamW', lr=6e-5, betas=(0.9, 0.999), weight_decay=0.05),\n",
    "    constructor='LayerDecayOptimizerConstructor_ViT', \n",
    "    paramwise_cfg=dict(\n",
    "        num_layers=12, \n",
    "        layer_decay_rate=0.9,\n",
    "        )\n",
    "        )\n",
    "optimizer = build_optim_wrapper(model, optim_wrapper)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer.optimizer, max_epoch, eta_min=0, last_epoch=-1)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.to(device)\n",
    "count = 0\n",
    "best_loss = 99999\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_index =train_index.reshape(-1,)\n",
    "test_index =test_index.reshape(-1,)\n",
    "val_loss = 0\n",
    "train_loss = 0\n",
    "train_correct = 0\n",
    "train_total= 1\n",
    "val_correct = 0\n",
    "val_total= 0\n",
    "best_loss=999\n",
    "test_correct=0\n",
    "test_total=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_net = torch.load((r\"spat-base.pth\"), map_location=torch.device('cpu'))\n",
    "model_params =model.state_dict()\n",
    "for k in list(per_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'pos_embed' in k:\n",
    "        del per_net['model'][k]\n",
    "same_parsms = {k: v for k, v in per_net['model'].items() if k in model_params.keys()}\n",
    "model_params.update(same_parsms)\n",
    "model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_hsi=torch.from_numpy(data).permute(2,0,1).to(device)\n",
    "img_hsi = img_hsi.to(torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "OA_train 0.1036468330134357\n",
      "loss_train 11.030953645706177\n",
      "OA_val 0.2744360902255639\n",
      "loss_val 10.625585556030273\n",
      "######################save model######################\n",
      "epoch 10\n",
      "OA_train 0.6371263765076036\n",
      "loss_train 2.652833878993988\n",
      "OA_val 0.5319548872180451\n",
      "loss_val 3.4561136960983276\n",
      "######################save model######################\n",
      "epoch 20\n",
      "OA_train 0.7897628422305649\n",
      "loss_train 0.3625493571162224\n",
      "OA_val 0.6478696741854637\n",
      "loss_val 2.143505334854126\n",
      "######################save model######################\n",
      "epoch 30\n",
      "OA_train 0.8552819304013398\n",
      "loss_train 0.1138099879026413\n",
      "OA_val 0.7039473684210527\n",
      "loss_val 2.6115793883800507\n",
      "epoch 40\n",
      "OA_train 0.8903428544627363\n",
      "loss_train 0.0538227679207921\n",
      "OA_val 0.737593984962406\n",
      "loss_val 2.9457483887672424\n",
      "epoch 50\n",
      "OA_train 0.9117680328796048\n",
      "loss_train 0.03354680631309748\n",
      "OA_val 0.7606516290726817\n",
      "loss_val 3.104219973087311\n",
      "epoch 60\n",
      "OA_train 0.9258850603701019\n",
      "loss_train 0.04052660055458546\n",
      "OA_val 0.776047261009667\n",
      "loss_val 2.99105703830719\n",
      "epoch 70\n",
      "OA_train 0.935944313534303\n",
      "loss_train 0.0339064602740109\n",
      "OA_val 0.7861842105263158\n",
      "loss_val 3.2192028164863586\n",
      "epoch 80\n",
      "OA_train 0.9438522352270838\n",
      "loss_train 0.011393201071768999\n",
      "OA_val 0.7940685045948204\n",
      "loss_val 3.4974238872528076\n",
      "epoch 90\n",
      "OA_train 0.9500221888802012\n",
      "loss_train 0.00854257948230952\n",
      "OA_val 0.8007518796992481\n",
      "loss_val 3.5453664660453796\n",
      "epoch 100\n",
      "OA_train 0.9549703927952629\n",
      "loss_train 0.0062493939767591655\n",
      "OA_val 0.8065618591934381\n",
      "loss_val 3.6266746520996094\n",
      "epoch 110\n",
      "OA_train 0.9590270438835086\n",
      "loss_train 0.004915387311484665\n",
      "OA_val 0.8110902255639098\n",
      "loss_val 3.7150421142578125\n",
      "epoch 120\n",
      "OA_train 0.9624131847872729\n",
      "loss_train 0.005087627563625574\n",
      "OA_val 0.814921920185078\n",
      "loss_val 3.7884169816970825\n",
      "epoch 130\n",
      "OA_train 0.9652823652030945\n",
      "loss_train 0.004168422718066722\n",
      "OA_val 0.817937701396348\n",
      "loss_val 3.8312569856643677\n",
      "epoch 140\n",
      "OA_train 0.9677036592517833\n",
      "loss_train 0.010766568826511502\n",
      "OA_val 0.8200501253132833\n",
      "loss_val 3.961488127708435\n",
      "epoch 150\n",
      "OA_train 0.9698424625259485\n",
      "loss_train 0.002558522071922198\n",
      "OA_val 0.8223684210526315\n",
      "loss_val 3.866159200668335\n",
      "epoch 160\n",
      "OA_train 0.9717155791259063\n",
      "loss_train 0.0022644829587079585\n",
      "OA_val 0.8244139761167625\n",
      "loss_val 3.9577072262763977\n",
      "epoch 170\n",
      "OA_train 0.9733696202246939\n",
      "loss_train 0.001982631685677916\n",
      "OA_val 0.8264411027568922\n",
      "loss_val 3.9864259362220764\n",
      "epoch 180\n",
      "OA_train 0.9748408962930696\n",
      "loss_train 0.0030855196819175035\n",
      "OA_val 0.8286505738029284\n",
      "loss_val 3.946023464202881\n",
      "epoch 190\n",
      "OA_train 0.9761581135912848\n",
      "loss_train 0.002777659203275107\n",
      "OA_val 0.830639097744361\n",
      "loss_val 4.04595810174942\n",
      "epoch 200\n",
      "OA_train 0.9773442657456396\n",
      "loss_train 0.0012916806881548837\n",
      "OA_val 0.8322592194772646\n",
      "loss_val 4.138576507568359\n",
      "epoch 210\n",
      "OA_train 0.9784179874408727\n",
      "loss_train 0.000994826652458869\n",
      "OA_val 0.833732057416268\n",
      "loss_val 4.168673098087311\n",
      "epoch 220\n",
      "OA_train 0.9793945405974539\n",
      "loss_train 0.0008801869116723537\n",
      "OA_val 0.8350768224910101\n",
      "loss_val 4.257130026817322\n",
      "epoch 230\n",
      "OA_train 0.9802865444010622\n",
      "loss_train 0.0010892031423281878\n",
      "OA_val 0.8363095238095238\n",
      "loss_val 4.2649285197258\n",
      "epoch 240\n",
      "OA_train 0.9811045235834377\n",
      "loss_train 0.000896564160939306\n",
      "OA_val 0.8374436090225564\n",
      "loss_val 4.296922564506531\n",
      "epoch 250\n",
      "OA_train 0.9818573256410845\n",
      "loss_train 0.0006251946106203832\n",
      "OA_val 0.8384904569115096\n",
      "loss_val 4.35775089263916\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(300+ 1):\n",
    "    count=0\n",
    "    train_loss = 0\n",
    "    for netinput_hsi,netinput_gt in train_loader:\n",
    "        model.train()\n",
    "        netinput_hsi = netinput_hsi.to(device)\n",
    "        b,c,h,w = netinput_hsi.shape\n",
    "        batch_pred = model(netinput_hsi)\n",
    "        batch_pred = batch_pred.reshape(b*img_size*img_size,-1)\n",
    "        netinput_gt = netinput_gt.reshape(-1)\n",
    "        available_label_idx = np.where(netinput_gt!=0)\n",
    "        if len(available_label_idx[0]) != 0:\n",
    "            available_label_idx= torch.from_numpy(available_label_idx[0]).to(device)\n",
    "            netinput_gt = netinput_gt.to(device)\n",
    "            netinput_gt = netinput_gt-1\n",
    "            batch_pred_loss = batch_pred[available_label_idx]\n",
    "            netinput_gt_loss = netinput_gt[available_label_idx]\n",
    "            loss = criterion(batch_pred_loss, netinput_gt_loss.long())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss = train_loss+loss.item()\n",
    "            y_pred = torch.argmax(batch_pred_loss, dim=1)\n",
    "            train_correct += (y_pred == netinput_gt_loss).sum().item()\n",
    "            train_total += netinput_gt_loss.size(0)\n",
    "            if count == 0:\n",
    "                y_pred_train =  y_pred.cpu().numpy()\n",
    "                y_gt_train = netinput_gt_loss.cpu().numpy()\n",
    "                count = 1\n",
    "            else:\n",
    "                y_pred_train = np.concatenate( (y_pred_train, y_pred.cpu().numpy()) )\n",
    "                y_gt_train = np.concatenate( (y_gt_train, netinput_gt_loss.cpu().numpy()) ) \n",
    "    OA_train = train_correct / train_total\n",
    "    if epoch%10==0:\n",
    "        count=0\n",
    "        val_loss = 0\n",
    "        torch.cuda.empty_cache()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            for netinput_hsi,netinput_gt in Val_loader:\n",
    "                netinput_hsi = netinput_hsi.to(device)\n",
    "                b,c,h,w = netinput_hsi.shape\n",
    "                batch_pred = model(netinput_hsi)\n",
    "                batch_pred = batch_pred.reshape(b*img_size*img_size,-1)\n",
    "                netinput_gt = netinput_gt.reshape(-1)\n",
    "                available_label_idx = np.where(netinput_gt!=0)\n",
    "                if len(available_label_idx[0]) != 0:\n",
    "                    available_label_idx= torch.from_numpy(available_label_idx[0]).to(device)\n",
    "                    netinput_gt = netinput_gt.to(device)\n",
    "                    netinput_gt = netinput_gt-1\n",
    "                    batch_pred_loss = batch_pred[available_label_idx]\n",
    "                    netinput_gt_loss = netinput_gt[available_label_idx]\n",
    "                    loss_val = criterion(batch_pred_loss, netinput_gt_loss.long())\n",
    "                    val_loss =val_loss+loss_val.item()\n",
    "                    y_pred = torch.argmax(batch_pred_loss, dim=1)\n",
    "                    val_correct += (y_pred == netinput_gt_loss).sum().item()\n",
    "                    val_total += netinput_gt_loss.size(0)\n",
    "                    if count == 0:\n",
    "                        y_pred_val =  y_pred.cpu().numpy()\n",
    "                        y_gt_val = netinput_gt_loss.cpu().numpy()\n",
    "                        count = 1\n",
    "                    else:\n",
    "                        y_pred_val = np.concatenate( (y_pred_val, y_pred.cpu().numpy()) )\n",
    "                        y_gt_val = np.concatenate( (y_gt_val, netinput_gt_loss.cpu().numpy()) )  \n",
    "            OA_val = val_correct / val_total \n",
    "\n",
    "        print('epoch',epoch)\n",
    "        print('OA_train',OA_train)\n",
    "        print('loss_train',train_loss)\n",
    "        print('OA_val',OA_val)\n",
    "        print('loss_val',val_loss)\n",
    "\n",
    "        if val_loss < best_loss :\n",
    "            best_loss = val_loss\n",
    "            print('######################save model######################')\n",
    "            torch.save(model.state_dict(), path_weight + r\"model_spat.pt\")\n",
    "        count=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test OA=0.7649\n",
      "kappa= [0.736728289550604]\n",
      "each_acc= [array([1.        , 0.71549894, 0.65153374, 0.9954955 , 0.84401709,\n",
      "       0.96503497, 1.        , 1.        , 1.        , 0.80041797,\n",
      "       0.55696721, 0.56401384, 0.98947368, 0.9712    , 0.98382749,\n",
      "       1.        ])]\n",
      "average_acc= [0.8773425274722919]\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load(path_weight + r\"model_spat.pt\"))\n",
    "    model.eval()\n",
    "    y_tes_pred_combine = np.zeros([height_orgin,width_orgin])\n",
    "    for j in range(len(Ly)):\n",
    "        for k in range(len(Lx)):\n",
    "            rStart, cStart = (Ly[j], Lx[k])\n",
    "            rEnd, cEnd = (rStart + input_size[0], cStart + input_size[1])\n",
    "            img_part_hsi = img_hsi[:,rStart:rEnd,cStart:cEnd].unsqueeze(0)\n",
    "            batch_pred = model(img_part_hsi)\n",
    "            batch_pred = torch.squeeze(batch_pred, 0)\n",
    "            batch_pred = batch_pred.detach().cpu().numpy()\n",
    "            batch_pred = np.argmax(batch_pred,1).reshape(img_size,img_size)\n",
    "            if j == 0 and k == 0:\n",
    "                y_tes_pred_combine[rStart:rEnd, cStart:cEnd] = batch_pred\n",
    "            elif j == 0 and k > 0:\n",
    "                y_tes_pred_combine[rStart:rEnd, cStart + int(overlap_size / 2):cEnd] = batch_pred[:,\n",
    "                                                                                    int(overlap_size / 2):]\n",
    "            elif j > 0 and k == 0:\n",
    "                y_tes_pred_combine[rStart + int(overlap_size / 2):rEnd, cStart:cEnd] = batch_pred[\n",
    "                                                                                    int(overlap_size / 2):,\n",
    "                                                                                    :]\n",
    "            else:\n",
    "                y_tes_pred_combine[rStart + int(overlap_size / 2):rEnd,\n",
    "                cStart + int(overlap_size / 2):cEnd] = batch_pred[int(overlap_size / 2):,\n",
    "                                                            int(overlap_size / 2):]\n",
    "\n",
    "y_tes_pred_combine =torch.from_numpy(y_tes_pred_combine).type(torch.LongTensor).to(device)+1\n",
    "overall_acc,OA_hi1,average_acc,kappa,each_acc=utils.evaluate_performance_all(y_tes_pred_combine.reshape(height_orgin*width_orgin), test_samples_gt, test_gt_onehot,  height_orgin, width_orgin, class_num, test_gt,device, require_AA_KPP=True, printFlag=False)\n",
    "print(\"test OA={:.4f}\".format(overall_acc))\n",
    "print('kappa=',kappa)\n",
    "print('each_acc=',each_acc)\n",
    "print('average_acc=',average_acc)\n",
    "testOA_combine = evaluate_performance(y_tes_pred_combine.reshape(height_orgin*width_orgin), test_samples_gt, test_gt_onehot, zeros)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
