{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from model import ss_fusion_seg\n",
    "import torch\n",
    "from torch  import nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report,cohen_kappa_score\n",
    "from model import split_data,utils,create_graph\n",
    "from sklearn import metrics, preprocessing\n",
    "from mmengine.optim import build_optim_wrapper\n",
    "from mmcv_custom import custom_layer_decay_optimizer_constructor,layer_decay_optimizer_constructor_vit\n",
    "import random\n",
    "import os\n",
    "import torch.utils.data as Data\n",
    "import copy\n",
    "import scipy.io as sio\n",
    "import spectral as spy\n",
    "from collections import Counter\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataReader():\n",
    "    def __init__(self):\n",
    "        self.data_cube = None\n",
    "        self.g_truth = None\n",
    "\n",
    "    @property\n",
    "    def cube(self):\n",
    "        \"\"\"\n",
    "        origin data\n",
    "        \"\"\"\n",
    "        return self.data_cube\n",
    "\n",
    "    @property\n",
    "    def truth(self):\n",
    "        return self.g_truth\n",
    "\n",
    "    @property\n",
    "    def normal_cube(self):\n",
    "        \"\"\"\n",
    "        normalization data: range(0, 1)\n",
    "        \"\"\"\n",
    "        return (self.data_cube - np.min(self.data_cube)) / (np.max(self.data_cube) - np.min(self.data_cube))\n",
    "class dataRaw(DataReader):\n",
    "    def __init__(self):\n",
    "        super(dataRaw, self).__init__()\n",
    "        raw_data_package = sio.loadmat(r\"data/Indian_pines_corrected.mat\")\n",
    "        self.data_cube = raw_data_package[\"data\"].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    data = dataRaw().cube\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_size =8\n",
    "img_size = 128\n",
    "pca_components = 20\n",
    "class_num=16 \n",
    "max_epoch = 300\n",
    "batch_size = 64 \n",
    "path_weight = r\"weights//\"\n",
    "path_result = r\"result//\"\n",
    "data = load_data()\n",
    "height_orgin, width_orgin, bands = data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, pca = split_data.apply_PCA(data, num_components=pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "padding img: (256, 256, 20)\n"
     ]
    }
   ],
   "source": [
    "img_train, num_H, num_W,data = utils.Get_train_and_test_data_inference(img_size, data)\n",
    "height, width, bands = data.shape \n",
    "img_train = torch.from_numpy(img_train.transpose(0,3,1,2)).type(torch.FloatTensor) \n",
    "data_train = Data.TensorDataset(img_train)\n",
    "train_loader = Data.DataLoader(data_train, batch_size=num_H,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = torch.zeros([height_orgin * width_orgin]).to(device).float()\n",
    "model = ss_fusion_seg.SSFusionFramework(\n",
    "                img_size = img_size,\n",
    "                in_channels = pca_components,\n",
    "                patch_size=patch_size,\n",
    "                classes = class_num,\n",
    "                model_size='base'#The optional values are 'base','large' and 'huge'\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_params =model.state_dict()\n",
    "spat_net = torch.load((r\"spat-base.pth\"), map_location=torch.device('cpu'))\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_map' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'spat_output_maps' in k:\n",
    "        del spat_net['model'][k]\n",
    "for k in list(spat_net['model'].keys()):\n",
    "    if 'pos_embed' in k:\n",
    "        del spat_net['model'][k]\n",
    "spat_weights = {}\n",
    "prefix = 'spat_encoder.'\n",
    "for key, value in spat_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spat_weights[new_key] = value\n",
    "per_net = torch.load((r\"spec-base.pth\"), map_location=torch.device('cpu'))\n",
    "model_params =model.state_dict()\n",
    "for k in list(per_net['model'].keys()):\n",
    "    if 'patch_embed.proj' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'spat_map' in k:\n",
    "        del per_net['model'][k]\n",
    "    if 'fpn1.0.weight' in k:\n",
    "        del per_net['model'][k]\n",
    "spec_weights = {}\n",
    "prefix = 'spec_encoder.'\n",
    "for key, value in per_net['model'].items():\n",
    "    new_key = prefix + key\n",
    "    spec_weights[new_key] = value\n",
    "model_params =model.state_dict()\n",
    "for k in list(spec_weights.keys()):\n",
    "    if 'spec_encoder.patch_embed' in k:\n",
    "        del spec_weights[k]\n",
    "merged_params = {**spat_weights, **spec_weights}\n",
    "same_parsms = {k: v for k, v in merged_params.items() if k in model_params.keys()}\n",
    "model_params.update(same_parsms)\n",
    "model.load_state_dict(model_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3526.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
      "  warnings.warn(\n",
      "/data/yao.jin/anaconda3/envs/pytorch/lib/python3.10/site-packages/torch/utils/checkpoint.py:61: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    pred = torch.zeros([num_W, num_H, class_num, img_size, img_size])\n",
    "    for batch_idx, (batch_data) in enumerate(train_loader):\n",
    "        for w in range(num_H):\n",
    "            netinput = batch_data[0][w]\n",
    "            netinput = torch.unsqueeze(netinput, 0).to(device)\n",
    "            batch_pred = model(netinput)\n",
    "            #batch_pred = batch_pred.detach()\n",
    "            batch_pred = batch_pred.reshape(img_size,img_size,-1)\n",
    "            batch_pred =batch_pred. permute(([2, 0, 1]), 0)\n",
    "            pred[batch_idx,w] = batch_pred\n",
    "    pred = torch.reshape(pred, [num_H, num_W, class_num, img_size, img_size])\n",
    "    pred = torch.permute(pred, [2, 0, 3, 1, 4])  # [2,num_H, img_size,num_W, img_size]]\n",
    "    pred = torch.reshape(pred, [class_num, num_H * img_size* num_W * img_size])\n",
    "    pred = torch.permute(pred, [1, 0])     \n",
    "    y =pred.to(device)\n",
    "    y_orgin =  utils.image_reshape(y,height,width,height_orgin,width_orgin,class_num)\n",
    "    y_orgin = torch.argmax(y_orgin, dim=1).cpu()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
